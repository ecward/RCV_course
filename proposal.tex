\documentclass[11pt,a4paper]{article}
\usepackage{listings}
\usepackage{hyperref}

\title{RCV Course Project Proposal}

\begin{document}

\maketitle

\section{DD3356-RCV2016 Overview}

This project on the RCV should is about integrating a number of modules to achieve some level of autonomy for the vehicle.

Course description:

\url{http://www.kth.se/student/kurser/kurs/DD3356?l=en}.

Course group page:

\url{https://www.kth.se/social/group/dd3356-rcv2016/}.

Course integration repository:

\url{https://github.com/ecward/RCV_course}.

\section{Project members}

\begin{itemize}
\item Nils Bore, CVAP KTH
\item Xi Chen, CVAP KTH
\item Silvia Cruciani, CVAP KTH
\item Johan Ekekrantz, CVAP KTH
\item Niclas Evestedt, Automatic Control LiU
\item Rui Oliveira, Automatic Control KTH
\item Erik Ward, CVAP KTH
\end{itemize}

\section{Background}

The Research Concept Vehicle (RCV) is a electric car developed at the
Integrated Transport Research Lab (ITRL) at KTH that is instrumented
with drive-by-wire capability and different sensors.

In addition to the work on assembling the hardware of the RCV, several
projects aiming to provide different functionality for the RCV have
been carried out, many of them master thesis projects. The aim of this
project is to integrate some of the components already developed for
the RCV with other software components, primarily open-source
components, using suitable middle-ware, in order to achieve
autonomy in some limited scenarios.

\section{Objectives}

In this project the aim is to provide sufficient autonomy for two
types of scenarios involving the RCV and other traffic participants.


\begin{enumerate}
\item  Parking lot (Open area) navigation. The RCV should handle navigating,
at low speeds, to a defined pose, e.g. a specific parking space, while
avoiding collisions with static obstacles and pedestrians. 


\item  Junction navigation. The RCV should successfully handle a T-junction
scenario where another car is driving through the junction and the
goal of the RCV is to make a left or right turn through the
junction. Here the speeds of the RCV and the other car is
significantly higher than in scenario 1., e.g. 10 m/s rather than
almost walking pace as in scenario 1.



\end{enumerate}

The most fundamental requirements for these scenarios is perception
algorithms that can estimate the drivable surface relative to the RCV,
localization with respect to a world coordinate frame where goal
way-points are described (where the left or right turn leads to in
scenario 2. and where the specific pose is in scenario 1.), motion
planing that avoids static obstacles and path-following control of the
vehicle. Both these scenarios requires detection and tracking of moving
objects, in scenario 2. another car and scenario 1. pedestrians, using
sensors mounted on the vehicle. For scenario 1. the detection range
requirement is shorter than in scenario 2.  however we need to be able
to detect much smaller targets.

Scenario 2. requires a model of where roads are and topology of the
road network, e.g. that the RCV should be in the right lane and this
leads to the right lane of the connecting road after the turn. A map
of the road network, such as an RNDF file or a Lanelet file, containing
the lon,lat of center of lanes is assumed to be available a-priori
although it might not be very accurate. We thus need to estimate the
RCVs position relative to the off-line road map, e.g. the distance to
the right edge of the road.

The motion planning in scenario 1. will be able to find a
route that does not collide with any static obstacle.  Consideration
of dynamic obstacles will be handled either by slowing down or by
considering dynamic obstacles during planning. Motion planning in
scenario 2.  is mostly concerned with small corrections with respect
the route given by the lane center in order to achieve a smooth
ride. Scenario 1. and 2. presents two different types of problems,
where in scenario 1. we need to perform precise manoeuvring at low
speeds where in scenario 2. we need to follow the road, rather than
finding a route, at higher speeds. In addition to avoiding obstacles
and dynamic objects generated speed profiles for the RCV car should
take into account comfort constraints such as maximal acceleration and
jerk.

While the route planning and speed planning might only consider a
simplified model of the vehicle, the employed lateral and longitudinal
control algorithms must provide stable and robust control and interface
with low-level control components of the vehicle.

Scenario 2. requires inference about the behavior of the other car,
what rules of the road applies and finding speed profiles that
correspond to safe, natural behavior of the RCV, e.g. break if the
other car has the right of way and only proceeding through the
intersection when the collision risk of doing so is negligible.

There will be a high level behavior module of the robot that
can figure out what kind of actions are appropriate, e.g. driving
slower on parking lots and according to speed limits on roads. This
high level behavior module should be able to take as input a mission
way-point and switch behaviors appropriately until the way point is
reached.

Most of the functionally that is required is reliant on accurate
state estimation of the RCV.

\section{Deliverables}

Demos at Arlanda Test Track that demonstrates that the integrated
components can handle the scenarios described in the intended
functionality section. For both types of scenarios, parking lot and junction driving, we assume we have driven the RCV manually
in the area where experiments are performed beforehand in order to
collect data to map the environment.

\begin{itemize}


\item Parking lot navigation with dynamic obstacles. The RCV should navigate at low speeds 
  to a defined goal pose, given as input from a user. Navigation
can include both forward an backward motion and the precision
required for successfully reaching the goal pose is set to be
sufficent for the RCV to get into normal size parking pocket. No
parallel parking is assumed to be necessary.

We assume that there are no major differences in the appereance of the
parking lot from the time the area was mapped. Only small differences
such as one or two parked cars which where not present during mapping
are assumed. For instance this would not allow there to be a
significant amount of snow present whilst the area was mapped during
dry, warm conditions.

The number of dynamic objects are assumed to be few and move at a 
walking pace, e.g. up to three pedestrians are walking on the parking lot.

\item Basic junction navigation. The RCV should successfully drive
  trough an empty T-junction, staying close to the center of the right
  lane leading up to the intersection and follow a natural path trough the intersection.
  If an object is blocking part of the road for the RCV it will deviate from the center of the
  lane, as long as we can stay within the road boundary, in order to pass the obstacle.

\item Junction navigation. The RCV should successfully negotiate a
  T-junction scenario where another car is driving through the
  junction and the goal of the RCV is to make a left or right turn
  through the junction. Here we assume that there are no
  other static obstacles on the road and only one other car is present, no other moving
objects such as pedestrains are considered here.

\end{itemize}

\section{Integration Milestones}

To make sure that our work towards the scenarios progress as planned and that
we do not get stuck at unforeseen obstacles, we plan to have several milestones
where we verify the gradual progress towards a working system.
In the first milestones we primarily aim to evaluate the basic navigation components
and message communication as the rest of the system relies on these components.

\subsection{Milestone 1: March 31, Data Collection}
\label{milestone1}

In this milestone we will evaluate several components of the underlying platform.

\begin{enumerate}
\item ROS Interfaces for the sensors on the car, including encoders, heading, GPS and IMU
\item ROS Interfaces for the perception components, i.e. message types and names
\item Establish that we can gather data from the car using ROS components for offline analysis
\item Decide which free space planner we will use for the parking lot scenario
\item A proof of concept of a path following algorithm
\end{enumerate}

\subsection{Milestone 2: April 30, Simple Demo \& Visualization}
\label{milestone2}

In the second milestone we will demonstrate some basic navigation capability.
The car will drive on a lane at the test track. At one point an
obstacl

\begin{enumerate}
\item Robot able to follow path provided by planner
\item Planner able to plan on obstacle costmap populated by global static map and laser
\item We are able to track dynamic obstacles in the Velodyne data
\item There is some humans and car detection from stereo images
\item Ground plane estimation is working
\end{enumerate}

\subsection{Milestone 3: May 31, Scenario Attempt}
\label{milestone3}

For this last milestone we will attempt both of the scenarios, enabling
us to verify the system performance and work out any remaining problems
before the final scenario demos.

\begin{enumerate}
\item Planner will work on costmap with static obstacles and dynamic obstacles from detection and tracking
\item GPS and point cloud/odometry localization are integrated
\item Stereo data is used to complete the Velodyne point cloud
\item Tracking in stereo and Velodyne data produce a joint output
\item Road/offroad labelling is integrated into the costmap
\end{enumerate}

\section{Work Packages}

\subsection{WP1 Platform} % and data interfaces

\begin{description}
\item[Participants] Erik, Johan, Rui
\item[Summary] Low level control and sensor interfaces via CAN are
  handled by GOAT (?)  a ? running XPC target. In order to interface
  with the rest of the system a ROS interface is to be developed such
  that sensor data is made available as ROS messages and an interface
  to the path following controller is available as a ROS topic. In this
  work package we will define ROS messages, and write the necessary
  software for converting messages sent over various Ethernet based
  protocols from GOAT to ROS messages. In addition, managing time
  synchronization, where a GPS device is used as a master clock for the
  system, is part of this work package.

  This work package also includes connecting the various computers and
  sensors and constructing mounting points for sensors on the RCV.
   
\item[Input] -
\item[Output] ROS messages for module communication
\item[Tasks for Integration Milestones]:\
	\begin{description}
		\item[Milestone 1] Define ROS interfaces and topics. Software to decode data from the GOAT. 

  Network Time Protocol (NTP) set up for time synchronization. 10 h
		\item[Milestone 2]
		\item[Milestone 3] 
	\end{description}	 
\end{description}

\subsection{WP2 Planner}

\begin{description}
\item[Participants] Erik, Niclas
\item[Summary] The planner workpackage is divided into two parts, one
  for structured environments where a well-defined road network is
  available and one for unstructured environments where no road
  network exists e.g. a parking lot. In structured environments parts
  of the approach presented by Werling \cite{werling2011optimal} will be implemented to
  handle dynamic traffic scenarios. A bundle of candidate trajectories
  are generated and scored according to a cost function and the best
  trajectory is sent to the low level controllers. In open
  environments a search algorithm will be used to generate an obstacle
  free path from the starting point to a goal point and this path will
  then be used by the Werling planner so dynamics obstacles can be
  handled.

  Dynamic objects will be handled by using trajectory predictions based 
  on tracked objects and selecting a suitable trajectory from the ones
  generated using the Werling planner.
  
\item[Input] Occupancy map (WP6), State estimation (WP4), Tracked objects (WP5)
\item[Output] Feasible trajectory for path following controller.
\item[Tasks for Integration Milestones]:\
	\begin{description}
		\item[Milestone 1]  Define ROS-interfaces for output
                  and input 5h \\
                  
                  Decide on a planner implementation to
                  use for unstructured area planning. Suggested
                  candidates are: Hybrid A* \cite{dolgov2010path} implementation from
                  current master thesis work by Karl Kurzer, iQMatic
                  RRT implementation or ROS move\_base planning
                  package. 5h  \\
                  
                  Implement a first version of the werling
                  planner that can handle static obstacles in a
                  simulation environment 30h
		\item[Milestone 2] 
		Simulation scenario from integration
                  point 2 to be run on real car but with simulated
                  static obstacles 40h \\
                  
                  Integration of planner for
                  unstructured environments started 10h                  
		\item[Milestone 3] Dynamic and static obstacles from
                  sensor data integrated in the Werling planner. 80h \\
                  
                  Planner for unstructured environments integrated 50h
	\end{description}	 
\end{description}

\subsection{WP3 Controller}

\begin{description}
\item[Participants] Lars, Rui
\item[Summary]
\item[Input]
\item[Output]
\item[Tasks for Integration Milestones]:\
	\begin{description}
		\item[Milestone 1]
		\item[Milestone 2]
		\item[Milestone 3] 
	\end{description}	 
\end{description}

\subsection{WP4 Localization \& Mapping}

\begin{description}
\item[Participants] Nils, Xi, Johan
\item[Summary]
\item[Input]
\item[Output]
\item[Tasks for Integration Milestones]:\
	\begin{description}
		\item[Milestone 1]
		\item[Milestone 2]
		\item[Milestone 3] 
	\end{description}	 
\end{description}

\subsection{Cameras} % calibration and depth estimation
\begin{description}
\item[Participants] Johan, Silvia
\item[Summary]

The RCV will contain two RGB cameras, set up for stereo using an arduino to perform the synchronized triggering of the cameras. The system will perform stereo matching using off-the-shelf available algorithms available in OpenCV and ROS. The RCV will also contain other sensors such as two Velodyne lidar sensors and a GPS. These sensors will have to be intrinsically and extrinsically calibrated to find the relative poses of the sensors in the RCV coordinate system. Solving these tasks, we look to apply previously developed work by Johan. 

\item[Input]
	Lidar pointclouds
	GPS position
\item[Output]
	TF of extrinsically calibrated sensor setup.
	Syncronized RGB images
	Disparity map/pointclouds for one or two of the RGB images, found using stereo matching
\item[Tasks for Integration Milestones]:\
	\begin{description}
		\item[Milestone 1]
			Data recording possible.
			Stereo setup in place, with synchronized image triggering.
		\item[Milestone 2]
			Disparity map/pointclouds created using stereo vision.
			Pointcloud registration usable to extrinsically calibrate lidar sensors available.
		\item[Milestone 3]
			Improved and tuned Disparity map/pointclouds created using stereo vision.
			Good estimate of stereo and lidar system TF.
	\end{description}	 
\end{description}

\subsection{WP5 Point Cloud Tracking}

\begin{description}
\item[Participants] Nils, Silvia
\item[Summary]
\item[Input]
\item[Output]
\item[Tasks for Integration Milestones]:\
	\begin{description}
		\item[Milestone 1]
		\item[Milestone 2]
		\item[Milestone 3] 
	\end{description}	 
\end{description}

\subsection{Detection} %

\begin{description}
\item[Participants] Silvia, Nils
\item[Summary]
	Using the data provided by the stereo camera system and the Velodyne Lidar sensors, objects and obstacles will be identified in the images and pointclouds. Objects of interest are cars and pedestrians. For detection in images, the solutions available exploiting the OpenCV library (e.g. built-in method for pedestrian detection) will be used. 
\item[Input]
	Lidar pointclouds and Stereo camera images.
\item[Output]
	Pose information of detected instances of obstacles and classes of objects of interest. Approximate extent of the objects in terms of a bounding box.
\item[Tasks for Integration Milestones]:\
	\begin{description}
		\item[Milestone 1]
			Data collection and ROS Interface setup.
		\item[Milestone 2]
			Detection of Objects in the stereo camera images through standard OpenCV algorithms. Detection of obstacles in the pointcloud.
		\item[Milestone 3]
			Fusion of the detection in the stereo camera images and in the Lidar pointcloud.
	\end{description}	 
\end{description}

\subsection{WP6 Semantic Labelling} %

\begin{description}
\item[Participants] Xi
\item[Summary]
\item[Input]
\item[Output]
\item[Tasks for Integration Milestones]:\
	\begin{description}
		\item[Milestone 1]
		\item[Milestone 2]
		\item[Milestone 3] 
	\end{description}	 
\end{description}

%\section{Integration and implementation}
%
%As far as possible we will use existing components to provide the
%required functionality. These components either come from previous
%work on the RCV, previous research code by project members or open
%source components. The components will be integrated using ROS as middle-ware.
%
%The perception requirements for the two scenarios represents the
%majority of the work required, and 4 out of the 7 project members will
%be focused on this.
%
%For localization and mapping, we will rely both on GPS position and the
%odometry and velodyne measurements from the RCV. Both the GPS and
%odometry will potentially require some amount of work to integrate into
%a ROS system. The velodyne is deemed more simple. Most likely, we will
%provide two separate localization systems with mapping running on its own, without
%integrating GPS coordinates. However, the system will be able to re-initialize
%using GPS coordinates if we can not track our position in the map.
%
%To localize using velodyne data, there are several different options.
%Considering the environment at the test site, robust localization will
%require that we use the full 3D data as opposed to a 2D slice.
%Importantly, we fill focus on localizing with respect to a pre-built map.
%The initial map will be built using an off-the-shelf SLAM approach.
%Then, one option is to use [insert Xi's ICP registration stuff]. However, this
%might need further adaptation to encompass the sparseness of the measurements.
%One approach that has proven to work in similar scenarios is the NDT-MCL
%framework from Örebro, \textit{Normal Distributions Transform Monte-Carlo Localization}
%by Saarinen et al., which uses the \textit{Normal Distribution
%Transform} for registration and \textit{Monte Carlo localization} for state estimation.
%Johan will work on registration of the velodyne data, something which might
%potentially be used for localization with respect to a point-based map.
%
%The scenarios also require that we have some tracking of dynamic objects.
%At the end of the project the system will be able to distinguish between
%static and dynamic elements in the map. Further, the dynamic elements should
%be separated into pedestrians and car. This might feed back into the planning
%system to adapt the behavior to different situations. Again, there are
%several possibilities for distinguishing between classes of objects in
%laser point clouds, especially with applications in driving scenarios.
%Ingmar Posner and his group have done a lot of work in this area and they
%have presented some systems that should be relatively straightforward to
%implement. In particular, they presented one paper last year that combined
%a method based on a kind of convolution in combination with a linear
%classifier. This can efficiently and precisely detect cars and
%pedestrians oriented along a few main directions, an assumption that should
%hold in our application scenario.
%There are several other benchmarks which perform better on the widely used
%\textit{Kitti} data set. One such example with code available is
%\textit{3D Object Proposals for Accurate Object Class Detection} from Toronto.
%Similar to other well performing methods, they employ neural nets for
%classification. Another example of a high scoring implementation with
%code available is \textit{Faster R-CNN: Towards Real-Time Object
%Detection with Region Proposal Networks} from University of Technology
%and Science of China and Microsoft Research.
%The plan is to integrate one of the neural net based systems is possible
%and if not implement a simple classifier like the from Posner's group.
%
%In order to avoid static obstacles around the vehicle we will use the
%cost map ROS component developed by Andreas Högger, which we will
%develop further in order to include only road as non-obstacles. The
%current plan is to use results from Karl Kurzer's master thesis on
%motion planning using the hybrid A* algorithm in open areas, however
%this might have to be revised depending on the outcome of his work.
%
%For trajectory planning in the presence of dynamic obstacles we will
%use the research code of Erik and Niclas which will use a Werling
%based approach which given a reference path, from the hybrid A* planner
%in the open area case and from the road-map otherwise, optimizes the
%robot's trajectory with respect to a cost function that avoids dynamic
%obstacles and minimizes jerk.
%
%For control of the RCV a trajectory tracking controller will be
%developed that has a useful and easy to use interface that will allow
%the RCV to be used reliably and easily through ROS. The Trajectory
%Tracking Controller will be able to receive trajectory requests and
%will be responsible for following them. In order to make the system
%reusable and useful for different purposes, the trajectory requests
%can come in the fol- lowing ways: An (x,y,) trajectory, i.e., a set of
%(x,y,) states with an associated time; An (x,y,) path with a fixed
%velocity; A GPS trajectory, with an associated time; A GPS path with a
%fixed velocity.
%
%A simple odometry/Information system will also be implemented, it will
%be responsible from broadcasting general information about the car
%such as its position and velocities from lower level components to
%ROS. Not much state estimation effort will be put into it, it will
%simply make use of the existing GPS/IMU to generate odometry
%information.

%\begin{itemize}
%\item 
%Nils and Xi: Integrate ROS NDT framework from Örebro in order to do
%mapping and localization of the vehicle.
%
%\item 
%Nils: Dynamic object tracking from velodyne data, possibly in
%combination with stereo data.
%
%\item 
%Xi: Road, Obstacle, Not Road classification of point clouds. That
%can be used to provide a local grid map for planning.
%
%\item 
%Erik: Offline road map, hard-coded as Lanelet file. Use of existing
%code from iQMatic project for this.
%
%\item 
%Silvia, Johan: Detection and tracking of moving objects using stereo
%camera
%
%\item 
%Johan: Point cloud registration for object modeling and mapping
%
%\item 
%Niclas, Erik: Trajectory planning, Werling, for risk averse behavior
%
%\item 
%Niclas: Integration of motion planner
%
%\item 
%Erik: Risk inference in intersection using probabilistic modeling of
%other vehicle's future motion.
%
%\item 
%Rui: Lateral/longitudinal path following control, interface with
%lower level systems of the RCV to ROS.
%
%\end{itemize}

\section{Learning outcomes}

\begin{itemize}
\item 
Erik: Understanding of possibilities and limitations of perception
algorithms based on velodyne data and state of the art computer
vision algorithms for use in higher level behaviors. For this
project my primary goal is to implement and test base-line methods
based on a time-gap model for junction navigation and if possible
test my own ideas based on simulation of driver behavior on the RCV.

\item 
Niclas: Previously my work has focused on path generation in
unstructured areas such as parking lots or open pit mining areas. In
this project I want to gain more understanding for methods for
trajectory generation in structured environments such as when
driving in a road network. I want to test the methods presented by
Werling to generate a vast amount of candidate trajectories than can
later be evaluated according to cost functions based on safety,
comfort, progress etc. A big part will be to find suitable cost
functions and integrate with the behavioral layer and risk
evaluation methods developed by Erik.

\item 
Silvia: Deep understanding of state of the art computer vision algorithms for
object detection and tracking, and learning how to apply to real problems and
for real time applications. For this project my goal is to implement detection
and tracking system, mainly for obstacle avoidance purpose, using a stereo camera system.

\item 
Johan: Understanding of possibilities and limitations of velodyne
lidar data as compared to structured light sensors such as
kinect/primesense sensors. I also seek to learn about other ways of
generating point-clouds and range data in the form of a stereo vision
systems. Given the different sensor characteristics, I want to learn
how to adapt and apply previously developed methods to new
environments. I also want to learn about some of the practical
details of producing an autonomous car, such as sensor placement and
calibration.

\item 
Xi: Understanding the state of the art algorithms for terrain point
cloud traversability classification. Compare the quality and
algorithm performance by using the point cloud generated from
velodyne (sparse, long range) and stereo camera (dense, short
range). Learning how to use multi-core/GPU programming to optimize
the program to run in real time for a fast driving car.

\item 
Nils: Understand more about how deep learning methods function, as applied
to detection scenarios, in this case cars, pedestrians and bicycles.
I also hope to learn more about the Velodyne sensor and how it might
be applicable to my work on indoor mobile robots.

\end{itemize}
\bibliographystyle{plain}
\bibliography{bibtexFile}

\end{document}

