\documentclass[11pt,a4paper]{article}
\usepackage{listings}
\usepackage{hyperref}
\usepackage[toc,page]{appendix}
\usepackage{graphicx}

\title{RCV Course Project Proposal}

\begin{document}

\maketitle

\section{DD3356-RCV2016 Overview}


This project proposal is for an autonomous vehicle project where a number of modules will be integrated on the Research Concept Vehicle (RCV) to achieve some level of autonomy.

Course description:

\url{http://www.kth.se/student/kurser/kurs/DD3356?l=en}.

Course group page:

\url{https://www.kth.se/social/group/dd3356-rcv2016/}.

Course integration repository:

\url{https://github.com/ecward/RCV_course}.

\section{Project members}

\begin{itemize}
\item Nils Bore, CVAP KTH
\item Xi Chen, CVAP KTH
\item Silvia Cruciani, CVAP KTH
\item Johan Ekekrantz, CVAP KTH
\item Niclas Evestedt, Automatic Control LiU
\item Rui Oliveira, Automatic Control KTH
\item Erik Ward, CVAP KTH
\item Lars Svensson, Mechatronics KTH
\end{itemize}

\section{Background}

The RCV is a electric car developed at the
Integrated Transport Research Lab (ITRL) at KTH that is instrumented
with drive-by-wire capability and different sensors.
In addition to the work on assembling the hardware of the RCV, several
projects aiming to provide different functionality for the RCV have
been carried out, many of them master thesis projects. The aim of this
project is to integrate some of the components already developed for
the RCV with other software components, primarily open-source
components, using suitable middle-ware, in order to achieve
autonomy in some limited scenarios.

\section{Objectives}

In this project the aim is to provide sufficient autonomy for two
types of scenarios involving the RCV and other traffic participants.


\begin{enumerate}
\item  Parking lot (Open area) navigation. The RCV should handle navigating,
at low speeds, to a defined pose, e.g. a specific parking space, while
avoiding collisions with static obstacles and pedestrians. 


\item  Junction navigation. The RCV should successfully handle a T-junction
scenario where another car is driving through the junction and the
goal of the RCV is to make a left or right turn through the
junction. Here the speeds of the RCV and the other car is
significantly higher than in scenario 1., e.g. 10 m/s rather than
almost walking pace as in scenario 1.



\end{enumerate}

The most fundamental requirements for these scenarios is perception
algorithms that can estimate the drivable surface relative to the RCV,
localization with respect to a world coordinate frame where goal
way-points are described
%(where the left or right turn leads to in scenario 2. and where the specific pose is in scenario 1.),
, motion planing that avoids static obstacles and path-following control of the
vehicle. Both these scenarios requires detection and tracking of moving
objects, in scenario 2, another car and scenario 1, pedestrians, using
sensors mounted on the vehicle. For scenario 1, the detection range
requirement is shorter than in scenario 2, however we need to be able
to detect much smaller targets.

Scenario 2, requires a model of where roads are and topology of the
road network, e.g. that the RCV should be in the right lane and this
leads to the right lane of the connecting road after the turn. A map
of the road network, such as an RNDF file or a Lanelet file, containing
the global position of the center of lanes is assumed to be available a-priori
although it might not be very accurate. We thus need to estimate the
RCVs position relative to the off-line road map, e.g. the distance to
the right edge of the road.

The motion planning in scenario 1, will be able to find a
route that does not collide with any static obstacle. Consideration
of dynamic obstacles will be handled either by slowing down or by
considering dynamic obstacles during planning. Motion planning in
scenario 2, is mostly concerned with small corrections with respect
the route given by the lane center in order to achieve a smooth
ride. Scenario 1 and 2 presents two different types of problems,
where in scenario 1, we need to perform precise manoeuvring at low
speeds where in scenario 2, we need to follow the road at higher speeds, rather than finding a route and slowly progressing through an unstructured environment. In addition to avoiding obstacles
and dynamic objects generated speed profiles for the RCV car should
take into account comfort constraints such as maximal acceleration and
jerk.

While the route planning and speed planning might only consider a
simplified model of the vehicle, the employed lateral and longitudinal
control algorithms must provide stable and robust control and interface
with low-level control components of the vehicle.

Scenario 2, requires inference about the behaviour of the other car,
what rules of the road applies and finding speed profiles that
correspond to safe, natural behaviour of the RCV, e.g. break if the
other car has the right of way and only proceeding through the
intersection when the collision risk of doing so is negligible.

There will be a high level behaviour module of the robot that
can figure out what kind of actions are appropriate, e.g. driving
slower on parking lots and according to speed limits on roads. This
high level behaviour module should be able to take as input a mission
way-point and switch behaviours appropriately until the way point is
reached.

Most of the functionally that is required is reliant on accurate
state estimation of the RCV.

\section{Deliverables}

Demonstrations at Arlanda Test Track that demonstrates that the integrated
components can handle the scenarios described in the intended
functionality section. For both types of scenarios, parking lot and junction driving, we assume we have driven the RCV manually
in the area where experiments are performed beforehand in order to
collect data to map the environment.

\begin{itemize}


\item Parking lot navigation with dynamic obstacles. The RCV should navigate at low speeds 
  to a defined goal pose, given as input from a user. Navigation
can include both forward an backward motion and the precision
required for successfully reaching the goal pose is set to be
sufficient for the RCV to get into normal size parking pocket. No
parallel parking is assumed to be necessary.

We assume that there are no major differences in the appearance of the
parking lot from the time the area was mapped. Only small differences
such as one or two parked cars which where not present during mapping
are assumed. For instance this would not allow there to be a
significant amount of snow present whilst the area was mapped during
dry, warm conditions. The number of dynamic objects are assumed to be few and move at a 
walking pace.
%, e.g. up to three pedestrians are walking on the parking lot.

\item Basic junction navigation. The RCV should successfully drive
  trough an empty T-junction, staying close to the center of the right
  lane leading up to the intersection and follow a natural path trough the intersection.
  If an object is blocking part of the road for the RCV it will deviate from the center of the
  lane, as long as we can stay within the road boundary, in order to pass the obstacle.

\item Junction navigation. The RCV should successfully negotiate a
  T-junction scenario where another car is driving through the
  junction and the goal of the RCV is to make a left or right turn
  through the junction. Here we assume that there are no
  other static obstacles on the road and only one other car is present, no other moving
objects such as pedestrians are considered here.

\end{itemize}

\section{Integration Milestones}
A number of intermediate milestones has been defined in order to verify the gradual progress towards a working system and to detect unforeseen problems early in the project.
%To make sure that our work towards the scenarios progress as planned and that
%we do not get stuck at unforeseen obstacles, we plan to have several milestones
%where we verify the gradual progress towards a working system.
In the first milestones we primarily aim to evaluate the basic navigation components
and message communication as the rest of the system relies on these components.

\subsection{Milestone 1: Data Collection, March 25}
\label{milestone1}

In this milestone several components of the underlying platform will be evaluated.
Most importantly, communication using ROS message protocols between computers and sensors needs to be tested and functioning.
%we will set up computers and sensors to communicate with each
%other using ROS message protocols. 
An important aspect of this is time-stamping
of individual messages. An evaluation of the use of the GPS-clock for time synchronisation will be done and also an evaluation to see if more than one GPS unit is needed.
The message types and topics for all the different sensors
will be decided and implemented at this point to enable for data collection at the Arlanda test track. The data should be saved in a ROS bag format, enabling
all project participants to simply play back the data to recreate the drive. \\ \\
The following explicit goals have been established for this milestone:
\begin{enumerate}
\item ROS Interfaces for the sensor data from the RCV platform. The sensor data includes wheel encoder data, position and heading from GPS and acceleration and angular velocity from IMU.
\item ROS Interfaces for the perception components, i.e. message types and names
\item Establish that we can gather data from the car using ROS components for offline analysis
\item Decide which free space planner we will use for the parking lot scenario
% Lars and Rui finds that this milestone will not be critical at this point
%\item A proof of concept of a path following algorithm
\end{enumerate}

\subsection{Milestone 2: Simple Demo \& Visualization, April 30}
\label{milestone2}

In the second milestone a demonstration of basic navigation and visualisation capability will be performed.
The car will drive on a lane at the test track and at one point a
simulated obstacle will appear along the route and the planner will have to take the obstacle into account and provide a path around for the path following controller. \\ \\
The milestone includes the following integration tasks:
\begin{enumerate}
\item Robot able to follow a simple path provided by the planner
\item Planner able to plan on obstacle costmap populated by global static map and laser
\item Tracking of dynamic obstacles from the Velodyne data
\item Basic human and car detection from stereo images
\item Ground plane estimation is working
\end{enumerate}

\subsection{Milestone 3: Scenario Integration, May 31}
\label{milestone3}

For this last milestone an attempt of both the scenarios will be performed to verify the system performance and work out any remaining problems before the final demonstration. This milestone will have a strong
focus on integration of existing components.

This final integration milestone enables for testing
of a more fully integrated system, including:
\begin{enumerate}
\item Planner will work on costmap with static obstacles and dynamic obstacles from detection and tracking
\item GPS and point cloud/odometry localization are integrated
\item Stereo data is used to complete the Velodyne point cloud
\item Tracking in stereo and Velodyne data produces a joint output
\item Road/offroad labelling is integrated into the costmap
\item Controller able to perform path following with satisfactory precision for paths with high velocity and turning rate
\end{enumerate}

\section{Work Packages}

\subsection{WP1 Platform} % and data interfaces

\begin{description}
\item[Participants] Erik, Johan, Rui, Lars
\item[Summary] In this work package the overall RCV-specific system architecture will be developed.

%The greater part of this WP will focus on correctly interfacing with the different sensors and actuators on the RCV, and making them available for use through a ROS interface.

In this WP the focus will be on correctly interfacing with the different sensors and actuators on the RCV, and making them available for use through a ROS interface.

This ROS interface will be developed such that sensor data is made available through ROS topics and an interface to the path following controller (to be developed in WP3) is available as a ROS service.

A detailed explanation of overall RCV system architecture to be developed in this WP is given in Appendix \ref{sec:system_structure_appendix}.

The issue of time synchronization between multiple computers on the RCV will be tackled. A GPS device will be set up such that it acts as a master clock for the whole system.

This work package also includes the mounting of the new sensors to be used in the RCV.

%This Sensor and actuator interfaces via CAN are
%  handled by real time computer running XPC target. In order to interface
%  with the rest of the system a ROS interface is to be developed such
%  that sensor data is made available as ROS messages and an interface
%  to the path following controller is available as a ROS topic. In this
%  work package we will define ROS messages, and write the necessary
%  software for converting messages sent over various Ethernet based
%  protocols from GOAT to ROS messages. In addition, managing time
%  synchronization, where a GPS device is used as a master clock for the
%  system, is part of this work package.

  %This work package also includes connecting the various computers and
  %sensors and constructing mounting points for sensors on the RCV.
   
\item[Input] -
\item[Output] Well defined ROS interface for sending path following tasks to the RCV and a functional ROS interface for accessing the vehicle sensor readings.
\item[Tasks for Integration Milestones]:\
	\begin{description}
		\item[Milestone 1] Well defined and functioning ROS interfaces for the RCV sensor readings. 40h\\
  Network Time Protocol (NTP) set up for time synchronization. 10 h\\
  Sensor mounts completed. 15 h 
		\item[Milestone 2] Well defined and functioning ROS interface for issuing trajectory commands to the RCV. 2h
		\item[Milestone 3] None
	\end{description}	 
\end{description}

\subsection{WP2 Planner}

\begin{description}
\item[Participants] Erik, Niclas
\item[Summary] The planner workpackage is divided into two parts, one
  for structured environments where a well-defined road network is
  available and one for unstructured environments where no road
  network exists e.g. a parking lot. In structured environments parts
  of the approach presented by Werling \cite{werling2011optimal} will be implemented to
  handle dynamic traffic scenarios. A bundle of candidate trajectories
  are generated and scored according to a cost function and the best
  trajectory is sent to the low level controllers. In open
  environments a search algorithm will be used to generate an obstacle
  free path from the starting point to a goal point and this path will
  then be used by the Werling planner so dynamics obstacles can be
  handled.

  Dynamic objects will be handled by using trajectory predictions based 
  on tracked objects and selecting a suitable trajectory from the ones
  generated using the Werling planner.
  
\item[Input] Occupancy map (WP6), State estimation (WP4), Tracked objects (WP5)
\item[Output] Feasible trajectory for path following controller.
\item[Tasks for Integration Milestones]:\
	\begin{description}
		\item[Milestone 1]  Define ROS-interfaces for output
                  and input 5h \\
                  
                  Decide on a planner implementation to
                  use for unstructured area planning. Suggested
                  candidates are: Hybrid A* \cite{dolgov2010path} implementation from
                  current master thesis work by Karl Kurzer, iQMatic
                  RRT implementation or ROS move\_base planning
                  package. 5h  \\
                  
                  Implement a first version of the werling
                  planner that can handle static obstacles in a
                  simulation environment 30h
		\item[Milestone 2] 
		Simulation scenario from integration
                  point 2 to be run on real car but with simulated
                  static obstacles 40h \\
                  
                  Integration of planner for
                  unstructured environments started 10h                  
		\item[Milestone 3] Dynamic and static obstacles from
                  sensor data integrated in the Werling planner. 80h \\
                  
                  Planner for unstructured environments integrated 50h
	\end{description}	 
\end{description}

\subsection{WP3 Controller}

\begin{description}
\item[Participants] Lars, Rui
\item[Summary]
The controller work package will have the main objective of providing a practical trajectory following control functionality. The controller will receive a path in UTM coordinates with a corresponding velocity profile or a time reference for every point of the path. If possible, the control strategy developed within the GCDC project will be modified and used, otherwise a new controller will be developed. Possible alternatives are found in \cite{werling2010tracking} and \cite{lima2015}. In practice, the work will be divided into three parts. Deciding on a control strategy, conditioning of input signals and controller implementation. 
\item[Input]
Feasible trajectory (WP2)
\item[Output] -
\item[Tasks for Integration Milestones]:\
	\begin{description}
		\item[Milestone 1] Decide on control strategy and signal conditioning 20h\\
		
			For this milestone we aim to have determined whether the GCDC controller can be reused 
			and have arrived at a clearly defined control strategy. Furthermore, the signal conditioning 
			of the inputs, i.e extracting control errors etc. from available data, shall be done. 30h
		\item[Milestone 2] Controller implementation done and ready for initial testing 50h
		\item[Milestone 3] Controller iterated and tuned 50h\\
		
			At this point the controller shall be performing satisfactorily for the demonstration
	\end{description}	 
\end{description}

\subsection{WP4 Localization \& Mapping}

\begin{description}
\item[Participants] Nils, Xi, Johan
\item[Summary] To be able to navigate along a road and in a parking lot, the car will have to know its 
			   position with respect to the road and its surroundings. It also needs to know if there
			   are obstacles blocking its movement so it can wait or adapt its movement.
			   To this end, a map of the test area using the point clouds from the
			   Velodyne need to be created. An initial map will be built in an offline manner, using GPS data and possibly
			   IMU and/or wheel odometry. A framework such as NDT-MCL \cite{saarinen2013normal} 
			   will be used to localize, as a recent study \cite{magnusson2015beyond} has demonstrated good results
			   in similar scenarios. If precision is not sufficient the work in e.g. \cite{Zhang_DEMO},
			   which combines the point clouds with visual information to provide odometry estimates might be used to increase the accuracy.
			   The position in the global map together
			   with the sensor reading will be combined to create an obstacle costmap for the planner.
			   In the end, and integration of the road/off-road labelling in the costmap will be done.
				
\item[Input] Point clouds, cameras, GPS, odometry, ground plane estimation, road/off-road labelling
\item[Output] Localization in global map, obstacle costmap
\item[Tasks for Integration Milestones]:\
	\begin{description}
		\item[Milestone 1] Data collection, construction of global map for localization 90h
		\item[Milestone 2] Localize in global map, provide costmap to planner 80h
		\item[Milestone 3] Integrate road labelling from WP8 50h
	\end{description}	 
\end{description}

\subsection{WP5 Cameras} % calibration and depth estimation
\begin{description}
\item[Participants] Johan, Silvia
\item[Summary]

The RCV will contain two RGB cameras, set up for stereo using an Arduino to perform the synchronized triggering of the cameras. The system will perform stereo matching using off-the-shelf available algorithms available in OpenCV and ROS. The RCV will also contain other sensors such as two Velodyne lidar sensors and a GPS. These sensors will have to be intrinsically and extrinsically calibrated to find the relative poses of the sensors in the RCV coordinate system. Solving these tasks, previously developed work in \cite{} will be applied. 

\item[Input]
	Lidar pointclouds, GPS position 
\item[Output]
	TF of extrinsically calibrated sensor setup.  \\
	Synchronized RGB images \\
	Disparity map/pointclouds for one or two of the RGB images, found using stereo matching \\
\item[Tasks for Integration Milestones]:\
	\begin{description}
		\item[Milestone 1]
			Data recording possible. 20 h\\
			
			Stereo setup in place, with synchronized image triggering. 60 h \\
		\item[Milestone 2]
			Disparity map/pointclouds created using stereo vision. 20 h \\
			
			Pointcloud registration usable to extrinsically calibrate lidar sensors available. 40 h \\
		\item[Milestone 3]
			Improved and tuned Disparity map/pointclouds created using stereo vision. 20 h \\
			
			Good estimate of stereo and lidar system TF. 20 h \\
	\end{description}	 
\end{description}

\subsection{WP6 Point Cloud Tracking}

\begin{description}
\item[Participants] Nils, Silvia
\item[Summary] The goal of this work package is to track dynamic obstacles from Velodyne data.
			   This is a capability needed for planning in the presence of cars, where a greater field of view is needed than the cameras can offer.
			   A recognition algorithm for cars and humans from the point clouds will be developed,
			   and results will be integrated with recognition and tracking from images.
\item[Input] Velodyne data, odometry, ground plane estimation
\item[Output] Recognition \& tracking of dynamic obstacles in Velodyne data
\item[Tasks for Integration Milestones]:\
	\begin{description}
		\item[Milestone 1] ROS Interfaces established, visualization components in place 50h
		\item[Milestone 2] Ground plane estimation integrated, basic tracking of dynamic obstacles 40h\\
		
						   Proof of concept implementation of recognition of humans and cars in point clouds building on \cite{wang2015voting}. 60h
		\item[Milestone 3] Fuse the detection in stereo images and point clouds. Provide dynamic obstacles
						   with velocity and heading to the planner. 20h
	\end{description}	 
\end{description}

\subsection{WP7 Detection} %

\begin{description}
\item[Participants] Silvia, Nils
\item[Summary]
	Using the data provided by the stereo camera system and the Velodyne Lidar sensors, objects and obstacles will be identified in the images and pointclouds. Objects of interest are cars and pedestrians. For detection in images, the solutions available exploiting the OpenCV library (e.g. built-in method for pedestrian detection) will be used. 
\item[Input]
	Lidar pointclouds and Stereo camera images.
\item[Output]
	Pose information of detected instances of obstacles and classes of objects of interest. Approximate extent of the objects in terms of a bounding box.
\item[Tasks for Integration Milestones]:\
	\begin{description}
		\item[Milestone 1]
			Data collection and ROS Interface setup. 20h
		\item[Milestone 2]
			Detection of objects in the stereo camera images through standard OpenCV algorithms. 20h \\
			
			Detection of obstacles in the pointcloud. 20h
		\item[Milestone 3]
			Fusion of the detection in the stereo camera images and in the Lidar pointcloud. 20h
	\end{description}	 
\end{description}

\subsection{WP8 Semantic Labelling} %

\begin{description}
\item[Participants] Xi
\item[Summary]
The objective of semantic labeling package is to identify the road/non-road area by classifying the point cloud from both velodyne and stereo camera. It will use the result of dynamic object detection from WP6 and WP7 and generate an occupancy map with only static obstacles for path planning. 
\item[Input]
Velodyne pointcloud, Stereo Pointcloud, dynamic obstacles (WP6, WP7)
\item[Output]
Static Occupancy map, labeled pointcloud
\item[Tasks for Integration Milestones]:\
\begin{description}
\item[Milestone 1]
Data collection and setting up ros interface. 20h
\item[Milestone 2]
Road area detection for both camera stereo and Velodyne. 50h\\ 

Static obstacle detection. 30h
\item[Milestone 3] 
Integrating the classification result with WP6, WP7 and generate static occupancy map. 50h
\end{description}  
\end{description}​

%\section{Integration and implementation}
%
%As far as possible we will use existing components to provide the
%required functionality. These components either come from previous
%work on the RCV, previous research code by project members or open
%source components. The components will be integrated using ROS as middle-ware.
%
%The perception requirements for the two scenarios represents the
%majority of the work required, and 4 out of the 7 project members will
%be focused on this.
%
%For localization and mapping, we will rely both on GPS position and the
%odometry and velodyne measurements from the RCV. Both the GPS and
%odometry will potentially require some amount of work to integrate into
%a ROS system. The velodyne is deemed more simple. Most likely, we will
%provide two separate localization systems with mapping running on its own, without
%integrating GPS coordinates. However, the system will be able to re-initialize
%using GPS coordinates if we can not track our position in the map.
%
%To localize using velodyne data, there are several different options.
%Considering the environment at the test site, robust localization will
%require that we use the full 3D data as opposed to a 2D slice.
%Importantly, we fill focus on localizing with respect to a pre-built map.
%The initial map will be built using an off-the-shelf SLAM approach.
%Then, one option is to use [insert Xi's ICP registration stuff]. However, this
%might need further adaptation to encompass the sparseness of the measurements.
%One approach that has proven to work in similar scenarios is the NDT-MCL
%framework from Örebro, \textit{Normal Distributions Transform Monte-Carlo Localization}
%by Saarinen et al., which uses the \textit{Normal Distribution
%Transform} for registration and \textit{Monte Carlo localization} for state estimation.
%Johan will work on registration of the velodyne data, something which might
%potentially be used for localization with respect to a point-based map.
%
%The scenarios also require that we have some tracking of dynamic objects.
%At the end of the project the system will be able to distinguish between
%static and dynamic elements in the map. Further, the dynamic elements should
%be separated into pedestrians and car. This might feed back into the planning
%system to adapt the behavior to different situations. Again, there are
%several possibilities for distinguishing between classes of objects in
%laser point clouds, especially with applications in driving scenarios.
%Ingmar Posner and his group have done a lot of work in this area and they
%have presented some systems that should be relatively straightforward to
%implement. In particular, they presented one paper last year that combined
%a method based on a kind of convolution in combination with a linear
%classifier. This can efficiently and precisely detect cars and
%pedestrians oriented along a few main directions, an assumption that should
%hold in our application scenario.
%There are several other benchmarks which perform better on the widely used
%\textit{Kitti} data set. One such example with code available is
%\textit{3D Object Proposals for Accurate Object Class Detection} from Toronto.
%Similar to other well performing methods, they employ neural nets for
%classification. Another example of a high scoring implementation with
%code available is \textit{Faster R-CNN: Towards Real-Time Object
%Detection with Region Proposal Networks} from University of Technology
%and Science of China and Microsoft Research.
%The plan is to integrate one of the neural net based systems is possible
%and if not implement a simple classifier like the from Posner's group.
%
%In order to avoid static obstacles around the vehicle we will use the
%cost map ROS component developed by Andreas Högger, which we will
%develop further in order to include only road as non-obstacles. The
%current plan is to use results from Karl Kurzer's master thesis on
%motion planning using the hybrid A* algorithm in open areas, however
%this might have to be revised depending on the outcome of his work.
%
%For trajectory planning in the presence of dynamic obstacles we will
%use the research code of Erik and Niclas which will use a Werling
%based approach which given a reference path, from the hybrid A* planner
%in the open area case and from the road-map otherwise, optimizes the
%robot's trajectory with respect to a cost function that avoids dynamic
%obstacles and minimizes jerk.
%
%For control of the RCV a trajectory tracking controller will be
%developed that has a useful and easy to use interface that will allow
%the RCV to be used reliably and easily through ROS. The Trajectory
%Tracking Controller will be able to receive trajectory requests and
%will be responsible for following them. In order to make the system
%reusable and useful for different purposes, the trajectory requests
%can come in the fol- lowing ways: An (x,y,) trajectory, i.e., a set of
%(x,y,) states with an associated time; An (x,y,) path with a fixed
%velocity; A GPS trajectory, with an associated time; A GPS path with a
%fixed velocity.
%
%A simple odometry/Information system will also be implemented, it will
%be responsible from broadcasting general information about the car
%such as its position and velocities from lower level components to
%ROS. Not much state estimation effort will be put into it, it will
%simply make use of the existing GPS/IMU to generate odometry
%information.

%\begin{itemize}
%\item 
%Nils and Xi: Integrate ROS NDT framework from Örebro in order to do
%mapping and localization of the vehicle.
%
%\item 
%Nils: Dynamic object tracking from velodyne data, possibly in
%combination with stereo data.
%
%\item 
%Xi: Road, Obstacle, Not Road classification of point clouds. That
%can be used to provide a local grid map for planning.
%
%\item 
%Erik: Offline road map, hard-coded as Lanelet file. Use of existing
%code from iQMatic project for this.
%
%\item 
%Silvia, Johan: Detection and tracking of moving objects using stereo
%camera
%
%\item 
%Johan: Point cloud registration for object modeling and mapping
%
%\item 
%Niclas, Erik: Trajectory planning, Werling, for risk averse behavior
%
%\item 
%Niclas: Integration of motion planner
%
%\item 
%Erik: Risk inference in intersection using probabilistic modeling of
%other vehicle's future motion.
%
%\item 
%Rui: Lateral/longitudinal path following control, interface with
%lower level systems of the RCV to ROS.
%
%\end{itemize}

\section{Learning outcomes}

\begin{itemize}
\item 
Erik: Understanding of possibilities and limitations of perception
algorithms based on Velodyne data and state of the art computer
vision algorithms for use in higher level behaviours. For this
project my primary goal is to implement and test base-line methods
based on a time-gap model for junction navigation and if possible
test my own ideas based on simulation of driver behaviour on the RCV.

\item 
Niclas: Previously my work has focused on path generation in
unstructured areas such as parking lots or open pit mining areas. In
this project I want to gain more understanding for methods for
trajectory generation in structured environments such as when
driving in a road network. I want to test the methods presented by
Werling \cite{werling2010tracking} to generate a vast amount of candidate trajectories than can
later be evaluated according to cost functions based on safety,
comfort, progress etc. A big part will be to find suitable cost
functions and integrate with the behavioural layer and risk
evaluation methods developed by Erik.

\item 
Silvia: Deep understanding of state of the art computer vision algorithms for
object detection and tracking, and learning how to apply to real problems and
for real time applications. For this project my goal is to implement detection
and tracking system, mainly for obstacle avoidance purpose, using a stereo camera system.

\item 
Johan: Understanding of possibilities and limitations of Velodyne
lidar data as compared to structured light sensors such as
Kinect/Primesense sensors. I also seek to learn about other ways of
generating point-clouds and range data in the form of a stereo vision
systems. Given the different sensor characteristics, I want to learn
how to adapt and apply previously developed methods to new
environments. I also want to learn about some of the practical
details of producing an autonomous car, such as sensor placement and
calibration.

\item 
Xi: Understanding the state of the art algorithms for terrain point
cloud traversability classification. Compare the quality and
algorithm performance by using the point cloud generated from
velodyne (sparse, long range) and stereo camera (dense, short
range). Learning how to use multi-core/GPU programming to optimize
the program to run in real time for a fast driving car.

\item 
Nils: Understand more about how deep learning methods function, as applied
to detection scenarios, in this case cars, pedestrians and bicycles.
I also hope to learn more about the Velodyne sensor and how it might
be applicable to my work on indoor mobile robots.

\item
Lars: I aim for further insights in two major areas. The first being the 
area of path and trajectory control of autonomous vehicles and the second being 
the control architecture and integration aspects concerning complex 
automated systems. Furthermore I hope to learn more about ROS in general
as well as the subjects of the other students. 

\item
Rui: In the past I have developed control algorithms for autonomous
car-like vehicles in safe scenarios (simulated and scaled environments), 
with this project with I hope to learn how to make these algorithms work
in the real world under real time constraints and with safety guarantees.
Furthermore I will have the opportunity to learn how can the interactions
with other systems , e.g., replanning while driving, might affect/destabilize
the controller performance.

\newpage
\begin{appendices}
\input{SystemStructureAppendix}
\end{appendices}


\end{itemize}
\bibliographystyle{plain}
\bibliography{bibtexFile}

\end{document}

